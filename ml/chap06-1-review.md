模型评估出问题的话后头的算法对于整体的准确性的帮助就无法评估

所以什么是模型评估？

# 传统评估方法
训练、验证、测试集

训练集：对模型参数估计
验证集：对超参数选择(例如对 L1 损失的 λ 的选择)
测试集：测试模型最终表现，可用来预测的

为什么还需要测试集？要验证泛化能力。如果在验证集上验证，超参数一定是对于验证集最优。

测试集替代验证集也是危险的。用测试集去测试，选择超参数，去测出模型参数。用一些非正常手段去猜，就能对测试集来说，完全预测正确。但这个的泛化能力可想而知。

当然，如上评估方式有什么问题。

有一个问题是，数据量是不够的。

同时训练集不够，精度不够。验证集不够，准确性不够。这两者的取舍是个问题。

但是吴恩达也有一个观点，可以大量数据组训练集，剩下一些做验证。但是这是需要验证集有很好的估计效果。

那么数据两个不够的时候怎么办？

# k - fold
kfold 将训练集平均分 k 份，其中一份为验证，其余为训练，循环 k 次。为了充分利用不多的数据集合。

测试集需要提前选好。

k-fold 准确率一般要取平均。

k - fold 一般随机选取。而 k - fold 和测试集依旧会有差别。而我们希望两者尽量一致

可能原因：
1. kfold 本身随机性
2. 训练与测试集本身差异。训练用北京的消费数据，但是测试集都是山东的消费数据。这种情况尤其在工作中，这种情况如果存在一定要提前说。

POC：概念验证。这么想：对一个问题用不同模型都跑一遍，结果看谁比较好。

在 kfold 和 test 比较一致的时候，还希望各 fold 之间用同样模型跑出来，效果应该差不多，可以是预测准确率等等。这里应该说的是各个被选中作为验证的 fold 被验证的时候效果应该差不多

kfold 怎么选？
1. k 少的话，训练就少。k = 3 和 k = 5 看看谁训练数据多就知道了。精度不够
2. k 多的，验证就少，准确不够。而且如果数据不少的时候，做 k fold 可能导致算多个模型，导致算力不足


# 分布匹配问题
训练验证测试集的分布，希望是一个的。但比如 19 年预测 20 年的经济，那疫情这种预测外的事情就导致严重错估。

较为好的应对方法：
1. 样本足够代表性。北京代表全国 -> 每个省抽样代表全国
2. 模型多样性：集成

那么集成思路？

# 集成思路
综合多个模型的共同结果，因为模型都有自己长短处。（怎么做这个，还没实践过，所以挖坑。）

模型集成举例：一个数据集 kfold 将其预测概率取平均。在比赛里这是单模型，但其实算多模型（理解为不同参数造不同模型，所以是一种集成）

用 kfold 选超参数(例如 λ)，在使用同样的参数训练，这样的问题是什么：超参数选择与观测数量关系很大。先用数据集 kfold 和用整个数据集，最佳参数是不一样的。

