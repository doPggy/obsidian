训练集为什么不能搞超参数。因为训练集要充分拟合，而不考虑泛化能力。而在验证集上，不管你的数据给多少，也只是看效果如何。 

过拟合的理解：身高去预测体重。为了拟合，身高可能潜在承担了太多其他指标，所以验证的时候能力不太行。

kfold 为什么要和测试集保持一致。kfold 的验证和测试集要是不太一致，说明泛化能力或者是哪个环节出问题了。

kfold 中每个参数不一定一样。

分箱是个啥？

GBDT 是一般的拟合模型，最终结果是得分。而随机森林或者决策树结果是分类。

本章的东西更多是需要实践，所以会存在不理解的地方。

# xgboost
xgboost 原理官网就有，但是看了一半！

# lightgbm
GOSS: gradient-based One-Side Sampling


# catboost
针对分类问题转化成 target mean。


# 代码实现准则-调参

## 树的复杂度
多和叶子个数相关，也有深度影响。leave wise growth 和 depth wise growth。

而且叶子个数与学习率高度相关

50 棵树过拟合和 1000 棵树过拟合哪个好？首先，学习率高，模型变化很大，学习率小，可能模型表现基本都差不多，或者收敛的慢。那么如果用很小学习率 1000 棵树，可能要跑很久，这个时候就要考虑算力问题。

树越复杂，叶子多的更容易过拟合。与之对应的学习率可能也要随之变化。

## 数据筛选随机性
随机选变量或观测。

## DART
类似 dropout 一种防止过拟合的方法，每次 boosting 的时候将前一轮树随机扔了。

容易减慢模型拟合结果。


## 还有很多参数如何寻找最优
在参数寻找的时候，有些模型是要考虑超参数还有变量选择。这个就涉及是否公平的问题了。举个例子，一开始使用 20 变量，a 超参数训练。后头又加了 20 个，却选择一样的超参数，并且不怎么调参，那么就不能说明这 20 个新加的有效，甚至效果还不如之前（比如过拟合）。

所以变量不一样了，参数或超参数也要变化，同时调参粒度是要一致的， 不然不能评估变量变化后的效果，这就是不公平。

参数寻找可以分成三个阶段（这些都是实践上的总结，一定需要实践）：
1. 随机探索。
2. 顺序搜索
3. 贝叶斯优化

### 随机探索
用一些极端方法，例如变量多多的。

记录各种 metric，收集模型运行信息。有时候会发现模型对某类指标好，另一个模型则对另外的指标效果好，这种信息就可以记录，并且可以考虑集成。

还可以检查变量重要性，比如树模型中，造了 20 个树，变量被选择次数哪些是最多的。

这样可能就能确定参数的合理范围然后就可以开始顺序搜索。

### 顺序搜索
可以按照参数的重要性开始一个个搜索了。（搜索，不断地调参)

例如先学习率+深度（树模型）。

这样尽量找一个合理范畴。尽量减少这个阶段给后头阶段带来的影响。

### Hyperopt
基于贝叶斯方法的优化。

已经调过的参数，范围缩小，没有调过的范围扩大。这个挺好理解的。已经调过的，应该是只有更小的偏差了。同时多次初始化效果比一次初始化跑多次来得好。(由于没用过，只能是概念上理解。后头希望能用上)

## 调参效果
要在 kfold 选取并验证。

有使用模型集成，最好构建 pipeline。


## 实践经验考
模型训练过程中展示训练和验证的误差，可以关心一下他们相比上一轮有明显下降时做了什么。

假如用 5-flod，要用多少课树，该如何选择？在一个 fold 上 300 达到最优，另一个 500 达到最优，那么取平均，就是 400 个。这个合理吗？那选最多的呢？选效果最好的时候的树的数量？


early stop：在验证集上误差没有减小就停止的验证次数。如果大，可能会又偏离最好的轮数或者是训练到结束，可能浪费算力还过拟合。如果小，可能就没逼近好结果就停止了。